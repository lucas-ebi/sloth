#!/usr/bin/env python3
"""
SLOTH Demo - Lazy by design. Fast by default.

Demonstrates parsing, validation, modification, and writing of mmCIF files
using SLOTH's ultra-simple API with high-performance gemmi backend.
"""

import argparse
import os
import json
import copy
import sys
import shutil
import traceback
from pathlib import Path
from sloth import (
    MMCIFHandler,
    ValidatorFactory,
    DataSourceFormat,
    SchemaValidator,
    SchemaValidatorFactory,
    ValidationError,
    PDBMLConverter,
    DictionaryParser,
    XMLSchemaValidator,
    RelationshipResolver,
    MMCIFToPDBMLPipeline,
    MMCIFParser,
)


def category_validator(category_name):
    """Example validator function."""
    print(f"‚úÖ Validating category: {category_name}")


def cross_checker(category_name_1, category_name_2):
    """Example cross-checker function."""
    print(f"üîó Cross-checking: {category_name_1} ‚Üî {category_name_2}")


def modify_data(mmcif):
    """Example data modification."""
    if not mmcif.data:
        print("‚ùå No data blocks found")
        return

    block = mmcif.data[0]
    print(f"üìã Working with block: {block.name}")

    # Try to modify database information
    if "_database_2" in block.categories:
        # Direct dot notation access - the most elegant way!
        db_category = block._database_2  # This is dot notation in action!
        if hasattr(db_category, "database_id") and db_category.database_id:
            original = db_category.database_id[-1]
            db_category.database_id[
                -1
            ] = "MODIFIED_DB"  # Simple assignment with dot notation
            print(f"‚úèÔ∏è  Modified database_id: '{original}' ‚Üí 'MODIFIED_DB'")
            print(
                f"   Using elegant dot notation: block._database_2.database_id[-1] = 'MODIFIED_DB'"
            )
        else:
            print("‚ÑπÔ∏è  No database_id found to modify")
    else:
        print("‚ÑπÔ∏è  No _database_2 category found")


def show_file_info(mmcif):
    """Display information about the parsed file."""
    print(f"\nüìä File Information:")
    print(f"   Data blocks: {len(mmcif.data)}")

    for i, block in enumerate(mmcif.data):
        print(f"   Block {i+1}: '{block.name}' with {len(block.categories)} categories")

        # Show first few categories
        category_names = block.categories[:5]
        if category_names:
            print(f"   Categories: {', '.join(category_names)}")
            if len(block.categories) > 5:
                print(f"   ... and {len(block.categories) - 5} more")


def demo_with_sample_file():
    """Create and demonstrate with a sample file."""
    sample_content = """data_1ABC
#
_entry.id 1ABC_STRUCTURE
#
_database_2.database_id      PDB
_database_2.database_code    1ABC
#
loop_
_atom_site.group_PDB
_atom_site.id
_atom_site.type_symbol
_atom_site.Cartn_x
_atom_site.Cartn_y
_atom_site.Cartn_z
ATOM   1    N  10.123 20.456 30.789
ATOM   2    C  11.234 21.567 31.890
ATOM   3    C  12.345 22.678 32.901
#
"""

    sample_file = "demo_structure.cif"
    with open(sample_file, "w") as f:
        f.write(sample_content)

    print(f"üìù Created sample file: {sample_file}")
    return sample_file


def demonstrate_2d_slicing(mmcif):
    """Demonstrate 2D slicing functionality with emphasis on dot notation."""
    if not mmcif.data:
        print("‚ùå No data blocks found")
        return

    block = mmcif.data[0]
    print(f"\nüî¢ Demonstrating 2D slicing with dot notation:")
    print(
        f"   The power of SLOTH's dot notation makes data access elegant and intuitive!"
    )

    # Find an appropriate category with multiple rows for demonstration
    demo_categories = ["_atom_site", "_entity_poly_seq", "_struct_conn"]
    demo_category = None

    for cat_name in demo_categories:
        if cat_name in block.categories:
            # Use dot notation to access category - this is the elegant way!
            if cat_name == "_atom_site":
                demo_category = block._atom_site  # Direct dot notation!
            elif cat_name == "_entity_poly_seq":
                demo_category = block._entity_poly_seq  # Direct dot notation!
            elif cat_name == "_struct_conn":
                demo_category = block._struct_conn  # Direct dot notation!

            if demo_category and demo_category.row_count >= 3:
                print(
                    f"   Using category: {cat_name} with {demo_category.row_count} rows"
                )
                print(f"   Accessed using elegant dot notation: block.{cat_name}")
                break

    if not demo_category:
        print("   No suitable category found with multiple rows for demonstration")
        return

    print("\nüìä Column-wise access with dot notation (the Pythonic way):")
    # Show all item names in the category
    print(f"   Available items: {', '.join(demo_category.items)}")

    # Get the first two item names for demonstration
    item_names = demo_category.items[:2]

    # Show dot notation access for typical items
    if "group_PDB" in demo_category.items:
        values = demo_category.group_PDB  # Direct dot notation!
        print(
            f"   demo_category.group_PDB: {values[:3]} {'...' if len(values) > 3 else ''}"
        )
    if "id" in demo_category.items:
        values = demo_category.id  # Direct dot notation!
        print(f"   demo_category.id: {values[:3]} {'...' if len(values) > 3 else ''}")
    if "type_symbol" in demo_category.items:
        values = demo_category.type_symbol  # Direct dot notation!
        print(
            f"   demo_category.type_symbol: {values[:3]} {'...' if len(values) > 3 else ''}"
        )

    # For comparison, show dictionary style access (less elegant)
    print("\n   Alternative dictionary access (less intuitive):")
    for item_name in item_names:
        values = demo_category[item_name]
        print(f"   demo_category['{item_name}']: {values[:3]}")

    print("\nüìã Row-wise access with dot notation (elegant and readable):")
    # Show first row with dot notation access
    first_row = demo_category[0]
    print(f"   Row 0 (clean dot notation access):")

    # Use direct dot notation for common attributes
    if "group_PDB" in demo_category.items:
        print(f"     row.group_PDB: {first_row.group_PDB}")  # Direct dot notation!
    if "id" in demo_category.items:
        print(f"     row.id: {first_row.id}")  # Direct dot notation!
    if "type_symbol" in demo_category.items:
        print(f"     row.type_symbol: {first_row.type_symbol}")  # Direct dot notation!

    # Show row.data
    print(f"\n   Complete row data as dictionary (row.data):")
    row_data = first_row.data
    for item_name, value in list(row_data.items())[:3]:
        print(f"     {item_name}: {value}")
    if len(row_data) > 3:
        print(f"     ... and {len(row_data) - 3} more items")

    # Show row slicing with dot notation access
    if demo_category.row_count >= 3:
        print(f"\nüìë Row slicing with dot notation:")
        rows = demo_category[0:3]
        print(f"   Slicing category[0:3] returns {len(rows)} rows")

        # Use dot notation for common attributes (the most elegant approach)
        if "group_PDB" in demo_category.items and "id" in demo_category.items:
            for i, row in enumerate(rows):
                # Direct dot notation access - this is the key pattern to highlight!
                print(f"   Row {i}: row.group_PDB={row.group_PDB}, row.id={row.id}")
        else:
            # Fallback for other attributes
            for i, row in enumerate(rows):
                item1 = item_names[0]
                item2 = item_names[1]
                print(
                    f"   Row {i}: row.{item1}={getattr(row, item1)}, row.{item2}={getattr(row, item2)}"
                )

    print("\nüí° Dot Notation Usage Tips (Pythonic best practices):")
    print("   1. Access data blocks: data.data_BLOCKNAME")
    print("   2. Access categories: block._category_name")
    print("   3. Access item values: category.item_name")
    print("   4. Access row values: row.item_name")
    print("   5. Complex example: data.data[0]._atom_site.Cartn_x[0]")
    print("   6. With slices: for row in category[0:3]: print(row.item_name)")
    print("\n   üí™ Dot notation makes your code more readable, elegant, and Pythonic!")


def demonstrate_export_functionality(mmcif, output_dir):
    """Demonstrate the new export functionality."""
    print(f"\nüìä Demonstrating export functionality:")

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Create handler
    handler = MMCIFHandler()

    # Export to JSON
    json_path = os.path.join(output_dir, "exported_data.json")
    handler.export_to_json(mmcif, json_path)
    print(f"   ‚úÖ Exported to JSON: {json_path}")

    # Export to XML
    xml_path = os.path.join(output_dir, "exported_data.xml")
    handler.export_to_xml(mmcif, xml_path)
    print(f"   ‚úÖ Exported to XML: {xml_path}")

    # Export to Pickle
    pickle_path = os.path.join(output_dir, "exported_data.pkl")
    handler.export_to_pickle(mmcif, pickle_path)
    print(f"   ‚úÖ Exported to Pickle: {pickle_path}")

    # Export to YAML (with try/except as it requires PyYAML)
    try:
        yaml_path = os.path.join(output_dir, "exported_data.yaml")
        handler.export_to_yaml(mmcif, yaml_path)
        print(f"   ‚úÖ Exported to YAML: {yaml_path}")
    except ImportError as e:
        print(f"   ‚ùå YAML export failed: {str(e)}")

    # Export to CSV (with try/except as it requires pandas)
    try:
        csv_dir = os.path.join(output_dir, "csv_files")
        
        # Clean the CSV directory to prevent cross-contamination from previous runs
        if os.path.exists(csv_dir):
            import shutil
            shutil.rmtree(csv_dir)
        
        file_paths = handler.export_to_csv(mmcif, csv_dir)
        print(f"   ‚úÖ Exported to CSV files in: {csv_dir}")
        # Show first CSV file path as example
        for block_name, categories in file_paths.items():
            if categories:
                first_category = next(iter(categories))
                first_path = categories[first_category]
                print(f"      Example: {os.path.basename(first_path)}")
                break
    except ImportError as e:
        print(f"   ‚ùå CSV export failed: {str(e)}")


def demonstrate_import_functionality(output_dir):
    """Demonstrate the new import functionality."""
    print(f"\nüì• Demonstrating import functionality:")

    # Create handler
    handler = MMCIFHandler()

    imported_containers = {}

    # Import from JSON
    json_path = os.path.join(output_dir, "exported_data.json")
    if os.path.exists(json_path):
        try:
            json_container = handler.import_from_json(json_path)
            imported_containers["JSON"] = json_container
            print(f"   ‚úÖ Imported from JSON: {json_path}")
            print(f"      Found {len(json_container.blocks)} data block(s)")
        except Exception as e:
            print(f"   ‚ùå JSON import failed: {str(e)}")

    # Import from XML
    xml_path = os.path.join(output_dir, "exported_data.xml")
    if os.path.exists(xml_path):
        try:
            xml_container = handler.import_from_xml(xml_path)
            imported_containers["XML"] = xml_container
            print(f"   ‚úÖ Imported from XML: {xml_path}")
            print(f"      Found {len(xml_container.blocks)} data block(s)")
        except Exception as e:
            print(f"   ‚ùå XML import failed: {str(e)}")

    # Import from Pickle
    pickle_path = os.path.join(output_dir, "exported_data.pkl")
    if os.path.exists(pickle_path):
        try:
            pickle_container = handler.import_from_pickle(pickle_path)
            imported_containers["Pickle"] = pickle_container
            print(f"   ‚úÖ Imported from Pickle: {pickle_path}")
            print(f"      Found {len(pickle_container.blocks)} data block(s)")
        except Exception as e:
            print(f"   ‚ùå Pickle import failed: {str(e)}")

    # Import from YAML (with try/except as it requires PyYAML)
    yaml_path = os.path.join(output_dir, "exported_data.yaml")
    if os.path.exists(yaml_path):
        try:
            yaml_container = handler.import_from_yaml(yaml_path)
            imported_containers["YAML"] = yaml_container
            print(f"   ‚úÖ Imported from YAML: {yaml_path}")
            print(f"      Found {len(yaml_container.blocks)} data block(s)")
        except ImportError as e:
            print(f"   ‚ùå YAML import failed: {str(e)}")
        except Exception as e:
            print(f"   ‚ùå YAML import failed: {str(e)}")

    # Import from CSV (with try/except as it requires pandas)
    csv_dir = os.path.join(output_dir, "csv_files")
    if os.path.exists(csv_dir):
        try:
            csv_container = handler.import_from_csv_files(csv_dir)
            imported_containers["CSV"] = csv_container
            print(f"   ‚úÖ Imported from CSV files in: {csv_dir}")
            print(f"      Found {len(csv_container.blocks)} data block(s)")
        except ImportError as e:
            print(f"   ‚ùå CSV import failed: {str(e)}")
        except Exception as e:
            print(f"   ‚ùå CSV import failed: {str(e)}")

    # Auto-detect format import demo
    if imported_containers:
        print(f"\nüîç Demonstrating auto-detect format import:")
        for format_name, first_path in [
            ("JSON", json_path),
            ("Pickle", pickle_path),
            ("YAML", yaml_path),
            ("XML", xml_path),
        ]:
            if os.path.exists(first_path) and format_name in imported_containers:
                try:
                    auto_container = handler.import_auto_detect(first_path)
                    print(
                        f"   ‚úÖ Auto-detected and imported from {format_name}: {first_path}"
                    )
                    print(f"      Found {len(auto_container.blocks)} data block(s)")

                    # Compare with direct import to verify consistency
                    original = imported_containers[format_name]
                    if len(auto_container.blocks) == len(original.blocks):
                        print(f"      ‚úì Content matches direct {format_name} import")
                    break
                except Exception as e:
                    print(f"   ‚ùå Auto-detect import failed for {format_name}: {str(e)}")

    return imported_containers


def demonstrate_round_trip(mmcif, imported_container, format_name):
    """Demonstrate round-trip validation between original and imported data."""
    print(f"\nüîÑ Demonstrating round-trip validation ({format_name}):")

    if not mmcif.data or not imported_container.data:
        print("   ‚ùå Missing data blocks for comparison")
        return False

    # Check if blocks match
    if len(mmcif.data) != len(imported_container.data):
        print(
            f"   ‚ùå Block count mismatch: Original={len(mmcif.data)}, Imported={len(imported_container.data)}"
        )
        return False

    # Compare first block
    original_block = mmcif.data[0]
    imported_block = imported_container.data[0]

    # Compare category count
    if len(original_block.categories) != len(imported_block.categories):
        print(
            f"   ‚ö†Ô∏è Category count differs: Original={len(original_block.categories)}, Imported={len(imported_block.categories)}"
        )

    # Verify key categories exist in both
    common_categories = set(original_block.categories).intersection(
        set(imported_block.categories)
    )
    print(f"   ‚úì Found {len(common_categories)} common categories")

    # Check a few sample values
    if common_categories:
        example_category = list(common_categories)[0]
        print(f"   üîç Checking values in category: {example_category}")

        original_cat = original_block[example_category]
        imported_cat = imported_block[example_category]

        # Compare item names
        original_items = set(original_cat.items)
        imported_items = set(imported_cat.items)
        common_items = original_items.intersection(imported_items)

        if common_items:
            sample_item = list(common_items)[0]
            original_values = original_cat[sample_item]
            imported_values = imported_cat[sample_item]

            # Check if array lengths match
            if len(original_values) == len(imported_values):
                print(
                    f"   ‚úì Item '{sample_item}' has {len(original_values)} values in both datasets"
                )

                # Sample check first value
                if original_values[0] == imported_values[0]:
                    print(f"   ‚úì First value matches: '{original_values[0]}'")
                else:
                    print(
                        f"   ‚ö†Ô∏è First value differs: Original='{original_values[0]}', Imported='{imported_values[0]}'"
                    )
            else:
                print(
                    f"   ‚ö†Ô∏è Value count differs: Original={len(original_values)}, Imported={len(imported_values)}"
                )

    print(f"   ‚úÖ Round-trip validation complete")
    return True


def demonstrate_schema_validation(mmcif, output_dir):
    """Demonstrate schema validation for different formats."""
    print(f"\nüõ°Ô∏è Demonstrating Schema Validation:")

    # Create temporary directory for validation examples
    validation_dir = os.path.join(output_dir, "validation_examples")
    os.makedirs(validation_dir, exist_ok=True)

    # ===== JSON Schema Validation =====
    print("\nüìù JSON Schema Validation Example:")

    # Get the path to the exported JSON file
    json_path = os.path.join(output_dir, "exported_data.json")

    if os.path.exists(json_path):
        try:
            # Create a JSON schema validator using the default schema
            json_validator = SchemaValidatorFactory.create_validator(
                DataSourceFormat.JSON
            )

            # Create a valid and an invalid data file
            with open(json_path, "r") as f:
                valid_data = json.load(f)

            # Copy the valid data
            invalid_data = copy.deepcopy(valid_data)

            # Create invalid data by removing all items in a category
            if invalid_data:
                block_name = list(invalid_data.keys())[0]
                if block_name in invalid_data:
                    category_name = list(invalid_data[block_name].keys())[0]
                    if category_name in invalid_data[block_name]:
                        # Make this category an empty object
                        if isinstance(invalid_data[block_name][category_name], dict):
                            invalid_data[block_name][category_name] = {}
                        # If it's an array, make it an empty array
                        elif isinstance(invalid_data[block_name][category_name], list):
                            invalid_data[block_name][category_name] = []

            # Save valid and invalid data
            valid_json_path = os.path.join(validation_dir, "valid_data.json")
            invalid_json_path = os.path.join(validation_dir, "invalid_data.json")

            with open(valid_json_path, "w") as f:
                json.dump(valid_data, f)

            with open(invalid_json_path, "w") as f:
                json.dump(invalid_data, f)

            # Validate valid data - should pass validation
            try:
                handler = MMCIFHandler()
                # Validate that the schema is correct for the valid data
                is_valid = json_validator.is_valid(valid_data)
                if is_valid:
                    valid_container = handler.import_from_json(
                        valid_json_path, schema_validator=json_validator
                    )
                    print(f"   ‚úÖ Valid JSON data passed validation")
                else:
                    print(f"   ‚ùå Valid JSON data failed pre-validation check")
            except ValidationError as e:
                print(f"   ‚ùå Unexpected validation error on valid data: {e}")
            except Exception as e:
                print(f"   ‚ùå Error processing valid JSON: {e}")

            # Validate invalid data - should fail validation
            try:
                # First check that the schema correctly identifies invalid data
                is_invalid = not json_validator.is_valid(invalid_data)
                if is_invalid:
                    print(f"   ‚úÖ Pre-validation correctly identified invalid JSON")
                    # This should raise a ValidationError
                    try:
                        json_validator.validate(invalid_data)
                        print(f"   ‚ùå Validation.validate() did not raise an error")
                    except ValidationError as e:
                        print(f"   ‚úÖ Validation.validate() correctly raised: {e}")
                else:
                    print(f"   ‚ùå Invalid JSON incorrectly passed pre-validation")

                # Now check that the import function correctly validates
                try:
                    invalid_container = handler.import_from_json(
                        invalid_json_path, schema_validator=json_validator
                    )
                    print(f"   ‚ùå Invalid JSON import did not raise an error")
                except ValidationError as e:
                    print(f"   ‚úÖ Import correctly failed: {e}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error during invalid JSON testing: {e}")

        except Exception as e:
            print(f"   ‚ùå JSON validation setup error: {e}")
    else:
        print(f"   ‚ö†Ô∏è JSON file not found, skipping validation")

    # ===== XML Schema Validation =====
    print("\nüìù XML Schema Validation Example:")

    # Get the path to the exported XML file
    xml_path = os.path.join(output_dir, "exported_data.xml")

    if os.path.exists(xml_path):
        try:
            # Create a simple XML validator using a function rather than XSD
            class SimpleXMLValidator(SchemaValidator):
                def validate(self, data):
                    return {"valid": True, "errors": []}

                def is_valid(self, data):
                    return True

            xml_validator = SimpleXMLValidator()

            # Create a valid copy for demonstration
            valid_xml_path = os.path.join(validation_dir, "valid_data.xml")
            with open(xml_path, "r") as src, open(valid_xml_path, "w") as dst:
                dst.write(src.read())

            # Validate valid data
            try:
                handler = MMCIFHandler()
                valid_container = handler.import_from_xml(
                    valid_xml_path, schema_validator=xml_validator
                )
                print(f"   ‚úÖ Valid XML data passed validation")
            except ValidationError as e:
                print(f"   ‚ùå Unexpected validation error: {e}")
            except Exception as e:
                print(f"   ‚ùå XML validation error: {str(e)}")

        except Exception as e:
            print(f"   ‚ùå XML validation setup error: {e}")
    else:
        print(f"   ‚ö†Ô∏è XML file not found, skipping validation")

    # ===== YAML Schema Validation =====
    print("\nüìù YAML Schema Validation Example:")

    # Get the path to the exported YAML file
    yaml_path = os.path.join(output_dir, "exported_data.yaml")

    if os.path.exists(yaml_path):
        try:
            # Create a YAML schema validator using the default schema
            yaml_validator = SchemaValidatorFactory.create_validator(
                DataSourceFormat.YAML
            )

            # Create a valid copy for demonstration
            valid_yaml_path = os.path.join(validation_dir, "valid_data.yaml")
            with open(yaml_path, "r") as src, open(valid_yaml_path, "w") as dst:
                dst.write(src.read())

            # Validate valid data
            try:
                handler = MMCIFHandler()
                valid_container = handler.import_from_yaml(
                    valid_yaml_path, schema_validator=yaml_validator
                )
                print(f"   ‚úÖ Valid YAML data passed validation")
            except ValidationError as e:
                print(f"   ‚ùå Unexpected validation error: {e}")
            except Exception as e:
                print(f"   ‚ùå YAML validation error: {str(e)}")

        except Exception as e:
            print(f"   ‚ùå YAML validation setup error: {e}")
    else:
        print(f"   ‚ö†Ô∏è YAML file not found, skipping validation")

    # ===== Auto-detect with validation =====
    print("\nüìù Auto-detect Format with Validation Example:")

    try:
        # Use one of the valid files with auto-detection
        auto_detect_path = (
            valid_json_path if locals().get("valid_json_path") else json_path
        )

        if os.path.exists(auto_detect_path):
            handler = MMCIFHandler()
            mmcif = handler.import_auto_detect(auto_detect_path, validate_schema=True)
            print(f"   ‚úÖ Auto-detected format and validated successfully")
        else:
            print(f"   ‚ö†Ô∏è File not found for auto-detection")

    except Exception as e:
        print(f"   ‚ùå Auto-detection/validation error: {e}")

    print("\nüõ°Ô∏è Schema validation demonstration completed")
    return validation_dir


def demonstrate_sample_data_creation():
    """Demonstrate both manual and programmatic approaches to creating sample data."""
    print("\nüìù Sample Data Creation Methods:")

    # Method 1: Manual file creation (like the existing demo)
    print("\nüñãÔ∏è  Method 1: Manual mmCIF file creation")
    sample_content = """data_1ABC
_entry.id 1ABC_STRUCTURE
_database_2.database_id PDB
_database_2.database_code 1ABC
loop_
_atom_site.group_PDB
_atom_site.id
_atom_site.type_symbol
_atom_site.Cartn_x
_atom_site.Cartn_y
_atom_site.Cartn_z
ATOM 1 N 10.123 20.456 30.789
ATOM 2 C 11.234 21.567 31.890
"""

    manual_file = "sample_manual.cif"
    with open(manual_file, "w") as f:
        f.write(sample_content)
    print(f"   ‚úÖ Created manual sample: {manual_file}")

    # Method 2: Programmatic creation using SLOTH's API with dictionary notation
    print("\n‚öôÔ∏è  Method 2: Programmatic creation using dictionary notation")
    try:
        from sloth.models import MMCIFDataContainer, DataBlock, Category

        # Create container and block
        mmcif = MMCIFDataContainer()
        block = DataBlock("1ABC")

        # Create categories and add data using dictionary-style assignment
        # Entry information
        entry_category = Category("_entry")
        entry_category["id"] = ["1ABC_STRUCTURE"]

        # Database information
        database_category = Category("_database_2")
        database_category["database_id"] = ["PDB"]
        database_category["database_code"] = ["1ABC"]

        # Atom site information
        atom_site_category = Category("_atom_site")
        atom_site_category["group_PDB"] = ["ATOM", "ATOM"]
        atom_site_category["id"] = ["1", "2"]
        atom_site_category["type_symbol"] = ["N", "C"]
        atom_site_category["Cartn_x"] = ["10.123", "11.234"]
        atom_site_category["Cartn_y"] = ["20.456", "21.567"]
        atom_site_category["Cartn_z"] = ["30.789", "31.890"]

        # Add categories to block
        block["_entry"] = entry_category
        block["_database_2"] = database_category
        block["_atom_site"] = atom_site_category

        # Add block to container
        mmcif["1ABC"] = block

        # Write using SLOTH
        programmatic_file = "sample_programmatic.cif"
        handler = MMCIFHandler()
        with open(programmatic_file, "w") as f:
            handler.file_obj = f
            handler.write(mmcif)
        print(f"   ‚úÖ Created programmatic sample: {programmatic_file}")

        # Method 3: NEW! Auto-creation with Elegant Dot Notation (README example)
        print(
            "\n‚ú® Method 3: ‚ú® Auto-creation with Elegant Dot Notation (README example)"
        )
        print(
            "   SLOTH can automatically create nested objects with elegant dot notation!"
        )

        # Create an empty container
        mmcif = MMCIFDataContainer()

        # Use dot notation to auto-create everything - just like in the README!
        mmcif.data_1ABC._entry.id = ["1ABC_STRUCTURE"]
        mmcif.data_1ABC._database_2.database_id = ["PDB"]
        mmcif.data_1ABC._database_2.database_code = ["1ABC"]

        # Add atom data
        mmcif.data_1ABC._atom_site.group_PDB = ["ATOM", "ATOM"]
        mmcif.data_1ABC._atom_site.id = ["1", "2"]
        mmcif.data_1ABC._atom_site.type_symbol = ["N", "C"]
        mmcif.data_1ABC._atom_site.Cartn_x = ["10.123", "11.234"]
        mmcif.data_1ABC._atom_site.Cartn_y = ["20.456", "21.567"]
        mmcif.data_1ABC._atom_site.Cartn_z = ["30.789", "31.890"]

        # Write using SLOTH (just like in the README)
        dot_notation_file = "sample_dot_notation.cif"
        with open(dot_notation_file, "w") as f:
            handler.file_obj = f
            handler.write(mmcif)
        print(f"   ‚úÖ Created dot notation sample: {dot_notation_file}")

        # Parse all files to verify they work
        manual_mmcif = handler.parse(manual_file)
        programmatic_mmcif = handler.parse(programmatic_file)
        auto_creation_mmcif = handler.parse(dot_notation_file)

        print(f"\nüîç Verification:")
        print(f"   Manual approach: {len(manual_mmcif.data[0].categories)} categories")
        print(
            f"   Dictionary approach: {len(programmatic_mmcif.data[0].categories)} categories"
        )
        print(
            f"   Dot notation approach: {len(auto_creation_mmcif.data[0].categories)} categories"
        )

        # Demonstrate the elegance of dot notation access
        print(f"\nüí° Demonstrating dot notation elegance:")
        print(
            f"   auto_creation_mmcif.data_1ABC._entry.id[0]: {auto_creation_mmcif.data_1ABC._entry.id[0]}"
        )
        print(
            f"   auto_creation_mmcif.data_1ABC._atom_site.type_symbol: {auto_creation_mmcif.data_1ABC._atom_site.type_symbol}"
        )

        return manual_file, programmatic_file, dot_notation_file

    except ImportError as e:
        print(f"   ‚ö†Ô∏è Programmatic approach not available: {e}")
        print("   üìã Using manual approach only")
        return manual_file, None, None
    except Exception as e:
        print(f"   ‚ùå Error in programmatic approach: {e}")
        return manual_file, None, None


def demonstrate_auto_creation():
    """Demonstrate the auto-creation feature as described in the README."""
    print("\nü™Ñ Auto-Creation Feature Demonstration")
    print("=" * 50)
    print("‚ú® SLOTH can automatically create nested objects with elegant dot notation!")
    print("   This is the exact example from the README.md file.\n")

    try:
        from sloth.models import MMCIFDataContainer
        from sloth.handler import MMCIFHandler

        # Create an empty container - this is all you need!
        print("üìù Creating an empty container...")
        mmcif = MMCIFDataContainer()
        print("   mmcif = MMCIFDataContainer()")

        # Use dot notation to auto-create everything
        print("\nüöÄ Using dot notation to auto-create everything...")
        print("   mmcif.data_1ABC._entry.id = ['1ABC_STRUCTURE']")
        mmcif.data_1ABC._entry.id = ["1ABC_STRUCTURE"]

        print("   mmcif.data_1ABC._database_2.database_id = ['PDB']")
        mmcif.data_1ABC._database_2.database_id = ["PDB"]

        print("   mmcif.data_1ABC._database_2.database_code = ['1ABC']")
        mmcif.data_1ABC._database_2.database_code = ["1ABC"]

        # Add atom data
        print("\nüß¨ Adding atom data...")
        print("   mmcif.data_1ABC._atom_site.group_PDB = ['ATOM', 'ATOM']")
        mmcif.data_1ABC._atom_site.group_PDB = ["ATOM", "ATOM"]

        print("   mmcif.data_1ABC._atom_site.type_symbol = ['N', 'C']")
        mmcif.data_1ABC._atom_site.type_symbol = ["N", "C"]

        print("   mmcif.data_1ABC._atom_site.Cartn_x = ['10.123', '11.234']")
        mmcif.data_1ABC._atom_site.Cartn_x = ["10.123", "11.234"]

        print("   mmcif.data_1ABC._atom_site.Cartn_y = ['20.456', '21.567']")
        mmcif.data_1ABC._atom_site.Cartn_y = ["20.456", "21.567"]

        print("   mmcif.data_1ABC._atom_site.Cartn_z = ['30.789', '31.890']")
        mmcif.data_1ABC._atom_site.Cartn_z = ["30.789", "31.890"]

        # Show what was created automatically
        print(f"\nüîç What was auto-created:")
        print(f"   üì¶ Container: {len(mmcif)} block(s)")
        print(f"   üß± Block '1ABC': {len(mmcif.data_1ABC.categories)} categories")
        print(f"   üìÇ Categories: {', '.join(mmcif.data_1ABC.categories)}")

        # Show elegant access
        print(f"\nüíé Elegant data access:")
        print(f"   Entry ID: {mmcif.data_1ABC._entry.id[0]}")
        print(f"   Database: {mmcif.data_1ABC._database_2.database_id[0]}")
        print(f"   Atom types: {mmcif.data_1ABC._atom_site.type_symbol}")
        print(f"   X coordinates: {mmcif.data_1ABC._atom_site.Cartn_x}")

        # Write using SLOTH
        print(f"\nüíæ Writing to file...")
        handler = MMCIFHandler()
        output_file = "auto_creation_demo.cif"
        with open(output_file, "w") as f:
            handler.file_obj = f
            handler.write(mmcif)
        print(f"   ‚úÖ Saved to: {output_file}")

        # Parse it back to verify
        print(f"\nüîÑ Verifying by parsing the file back...")
        parsed = handler.parse(output_file)
        print(f"   ‚úÖ Successfully parsed {len(parsed)} block(s)")
        print(f"   ‚úÖ Entry ID matches: {parsed.data_1ABC._entry.id[0]}")
        print(f"   ‚úÖ Atom count: {len(parsed.data_1ABC._atom_site.type_symbol)} atoms")

        print(f"\nüéâ Dot notation demonstration completed successfully!")
        print(f"üí° No manual DataBlock or Category creation required!")
        print(f"üöÄ Just write what you want, SLOTH creates what you need!")

        return output_file

    except Exception as e:
        print(f"   ‚ùå Error in auto-creation demonstration: {e}")
        import traceback

        traceback.print_exc()
        return None


def demo_backend_comparison(sample_file):
    """Demonstrate current gemmi backend vs legacy implementation"""
    print("\nüî¨ Handler Comparison - Gemmi Backend vs Legacy")
    print("=" * 50)
    
    try:
        # Test current handler (gemmi backend)
        print("‚ö° MMCIFHandler (gemmi backend - default):")
        current_handler = MMCIFHandler()
        current_mmcif = current_handler.parse(sample_file)
        current_atom_count = len(current_mmcif.data[0]._atom_site.Cartn_x) if '_atom_site' in current_mmcif.data[0].categories else 0
        print(f"   Parsed {current_atom_count} atoms")
        if current_atom_count > 0:
            print(f"   API: mmcif.data[0]._atom_site.Cartn_x[0] = {current_mmcif.data[0]._atom_site.Cartn_x[0]}")
        
        # Test legacy handler
        print("\nüìä Legacy MMCIFHandler (pure Python):")
        try:
            from sloth.legacy import MMCIFParser
            legacy_parser = MMCIFParser(None)
            legacy_mmcif = legacy_parser.parse_file(sample_file)
            legacy_atom_count = len(legacy_mmcif.data[0]._atom_site.Cartn_x) if '_atom_site' in legacy_mmcif.data[0].categories else 0
            print(f"   Parsed {legacy_atom_count} atoms")
            if legacy_atom_count > 0:
                print(f"   API: mmcif.data[0]._atom_site.Cartn_x[0] = {legacy_mmcif.data[0]._atom_site.Cartn_x[0]}")
            
            # Verify identical results
            if current_atom_count == legacy_atom_count:
                print("   ‚úÖ Identical atom counts!")
            else:
                print("   ‚ö†Ô∏è Different atom counts")
                
            if (current_atom_count > 0 and legacy_atom_count > 0 and 
                current_mmcif.data[0]._atom_site.Cartn_x[0] == legacy_mmcif.data[0]._atom_site.Cartn_x[0]):
                print("   ‚úÖ Identical first coordinate!")
            elif current_atom_count > 0 and legacy_atom_count > 0:
                print("   ‚ö†Ô∏è Different first coordinates")
            
            # Test write functionality
            print("\nüìù Testing write functionality with gemmi backend...")
            with open("current_write_test.cif", 'w') as f:
                current_handler.file_obj = f
                current_handler.write(current_mmcif)
            print("   ‚úÖ Successfully wrote file using gemmi backend")
            
            # Verify round-trip
            mmcif_roundtrip = current_handler.parse("current_write_test.cif")
            if mmcif_roundtrip.data[0]._entry.id[0] == current_mmcif.data[0]._entry.id[0]:
                print(f"   ‚úÖ Round-trip test: {mmcif_roundtrip.data[0]._entry.id[0]}")
            
            # Clean up
            if os.path.exists("current_write_test.cif"):
                os.remove("current_write_test.cif")
                
        except ImportError:
            print("   ‚ùå Legacy parser not available")
            print("   This should not happen as legacy is included by default")
            
    except Exception as e:
        print(f"   ‚ùå Error in comparison: {e}")


def create_pdbml_demo_data():
    """Create a comprehensive demo mmCIF file for PDBML conversion."""
    demo_content = """data_DEMO
#
_entry.id DEMO

#
_database_2.database_id      PDB
_database_2.database_code    DEMO

#
loop_
_citation.id
_citation.title
_citation.journal_abbrev
_citation.journal_volume
_citation.page_first
_citation.page_last
_citation.year
_citation.country
_citation.journal_id_ISSN
primary
'Structure determination by X-ray crystallography'
'Nature'
'450'
'123'
'130'
'2008'
'UK'
'0028-0836'
2
'Computational analysis of protein structures'
'Science'
'320'
'456'
'462'
'2007'
'US'
'0036-8075'

#
loop_
_citation_author.citation_id
_citation_author.name
_citation_author.ordinal
primary 'Smith, J.A.' 1
primary 'Johnson, K.L.' 2
primary 'Brown, M.R.' 3
2 'Davis, P.Q.' 1
2 'Wilson, S.T.' 2

#
loop_
_entity.id
_entity.type
_entity.src_method
_entity.pdbx_description
_entity.formula_weight
_entity.pdbx_number_of_molecules
1 polymer man 'Protein Chain A' 15486.2 1
2 polymer man 'Protein Chain B' 15486.2 1
3 non-polymer man 'WATER' 18.015 245

#
loop_
_struct_asym.id
_struct_asym.pdbx_blank_PDB_chainid_flag
_struct_asym.pdbx_modified
_struct_asym.entity_id
_struct_asym.details
A N N 1 ?
B N N 2 ?

#
loop_
_atom_type.symbol
_atom_type.number_in_cell
_atom_type.scat_dispersion_real
_atom_type.scat_dispersion_imag
N 1 0.0061 0.0033
C 1 0.0033 0.0016
O 1 0.0106 0.0060

#
loop_
_chem_comp.id
_chem_comp.type
_chem_comp.mon_nstd_flag
_chem_comp.name
_chem_comp.formula
_chem_comp.formula_weight
MET 'L-peptide linking' y METHIONINE 'C5 H11 N O2 S' 149.211

#
loop_
_atom_site.group_PDB
_atom_site.id
_atom_site.type_symbol
_atom_site.label_atom_id
_atom_site.label_alt_id
_atom_site.label_comp_id
_atom_site.label_asym_id
_atom_site.label_entity_id
_atom_site.label_seq_id
_atom_site.pdbx_PDB_ins_code
_atom_site.Cartn_x
_atom_site.Cartn_y
_atom_site.Cartn_z
_atom_site.occupancy
_atom_site.B_iso_or_equiv
_atom_site.pdbx_formal_charge
_atom_site.auth_seq_id
_atom_site.auth_comp_id
_atom_site.auth_asym_id
_atom_site.auth_atom_id
_atom_site.pdbx_PDB_model_num
_atom_site.U_iso_or_equiv
ATOM   1    N  N   . MET A 1 1   ? 20.154  6.718   46.973  1.00 25.00 0 1   MET A N   1 0.0316
ATOM   2    C  CA  . MET A 1 1   ? 21.618  6.765   47.254  1.00 24.50 0 1   MET A CA  1 0.0309
ATOM   3    C  C   . MET A 1 1   ? 22.147  8.178   47.451  1.00 23.85 0 1   MET A C   1 0.0301
ATOM   4    O  O   . MET A 1 1   ? 21.393  9.133   47.651  1.00 24.52 0 1   MET A O   1 0.0310

#
_struct.entry_id DEMO
_struct.title 'DEMONSTRATION STRUCTURE FOR PDBML PIPELINE'
_struct.pdbx_descriptor 'DEMO PROTEIN'

#
_exptl.entry_id DEMO
_exptl.method 'X-RAY DIFFRACTION'
_exptl.crystals_number 1
"""
    
    with open('pdbml_demo.cif', 'w') as f:
        f.write(demo_content)
    
    return 'pdbml_demo.cif'


def demonstrate_pdbml_pipeline(comprehensive=False):
    """Demonstrate the PDBML conversion pipeline."""
    print("\nüß¨ PDBML Conversion Pipeline Demo")
    print("=" * 40)
    
    if comprehensive:
        print("üìä Running comprehensive PDBML pipeline demonstration")
        demo_file = create_pdbml_demo_data()
    else:
        print("üìä Running basic PDBML pipeline demonstration")
        # Create simple demo data
        simple_content = """data_1ABC
#
_entry.id 1ABC

#
_database_2.database_id      PDB
_database_2.database_code    1ABC

#
loop_
_citation.id
_citation.title
_citation.journal_abbrev
primary
'Crystal structure of example protein'
'Nature'

#
loop_
_citation_author.citation_id
_citation_author.name
_citation_author.ordinal
primary 'Smith, J.A.' 1
primary 'Johnson, K.L.' 2
"""
        demo_file = 'simple_pdbml_demo.cif'
        with open(demo_file, 'w') as f:
            f.write(simple_content)
    
    try:
        # Step 1: Parse mmCIF
        print(f"\nüîç Step 1: Parsing mmCIF file ({demo_file})")
        parser = MMCIFParser(validator_factory=None)
        container = parser.parse_file(demo_file)
        
        print(f"   ‚úÖ Parsed successfully")
        print(f"   üìã Data blocks: {len(container.data)}")
        print(f"   üìã Block name: {container.data[0].name}")
        print(f"   üìã Categories: {len(container.data[0].categories)}")
        category_names = list(container.data[0].categories)
        print(f"   üìÇ Categories: {', '.join(category_names)}")
        
        # Step 2: Convert to PDBML XML
        print(f"\nüîÑ Step 2: Converting to PDBML XML")
        dict_path = Path(__file__).parent / "sloth" / "schemas" / "mmcif_pdbx_v50.dic"
        converter = PDBMLConverter(dictionary_path=dict_path)
        xml_content = converter.convert_to_pdbml(container)
        
        print(f"   ‚úÖ XML generated successfully")
        print(f"   üìÑ XML size: {len(xml_content)} characters")
        
        # Step 3: Validate XML against schema
        print(f"\nüõ°Ô∏è  Step 3: Validating XML against PDBX schema")
        schema_path = Path(__file__).parent / "sloth" / "schemas" / "pdbx-v50.xsd"
        
        is_valid = False
        errors = []
        
        if schema_path.exists():
            validator = XMLSchemaValidator(schema_path)
            validation_result = validator.validate(xml_content)
            
            is_valid = validation_result["valid"]
            errors = validation_result.get("errors", [])
            
            print(f"   {'‚úÖ' if is_valid else '‚ö†Ô∏è'} Validation: {'PASSED' if is_valid else 'FAILED'}")
            print(f"   üìù Total errors: {len(errors)}")
            
            if errors and len(errors) <= 5:
                print("   üîç Errors:")
                for i, error in enumerate(errors, 1):
                    print(f"      {i}. {error}")
            elif errors and len(errors) > 5:
                print("   üîç First 3 errors:")
                for i, error in enumerate(errors[:3], 1):
                    print(f"      {i}. {error}")
                print(f"      ... and {len(errors) - 3} more")
        else:
            print("   ‚ùå Schema file not found")
        
        # Step 4: Resolve relationships (only for comprehensive demo)
        if comprehensive:
            print(f"\nüîó Step 4: Resolving parent-child relationships")
            resolver = RelationshipResolver()
            nested_json = resolver.resolve_relationships(xml_content)
            
            print(f"   ‚úÖ Relationships resolved")
            print(f"   üìä Root categories: {len(nested_json)}")
            
            # Show relationship structure
            for cat_name, cat_data in list(nested_json.items())[:3]:
                if isinstance(cat_data, dict):
                    print(f"   üìÇ {cat_name}: {len(cat_data)} items")
                    if cat_data:
                        first_key = list(cat_data.keys())[0]
                        first_item = cat_data[first_key]
                        if isinstance(first_item, dict):
                            child_cats = [k for k, v in first_item.items() if isinstance(v, list)]
                            if child_cats:
                                print(f"      ‚îî‚îÄ‚îÄ Child categories: {', '.join(child_cats)}")
        
        # Step 5: Save outputs
        print(f"\nüíæ Step 5: Saving outputs")
        output_dir = Path("exports") / "pdbml_demo"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save XML
        xml_file = output_dir / "demo.xml"
        with open(xml_file, 'w', encoding='utf-8') as f:
            f.write(xml_content)
        print(f"   üíæ XML: {xml_file}")
        
        # Save validation report
        report_file = output_dir / "validation_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"PDBML XML Validation Report\n")
            f.write(f"===========================\n\n")
            f.write(f"Status: {'PASSED' if is_valid else 'FAILED'}\n")
            f.write(f"Total errors: {len(errors)}\n\n")
            if errors:
                f.write("Errors:\n")
                for i, error in enumerate(errors, 1):
                    f.write(f"  {i}. {error}\n")
        print(f"   üìã Report: {report_file}")
        
        if comprehensive:
            # Save nested JSON
            json_file = output_dir / "nested_relationships.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(nested_json, f, indent=2)
            print(f"   üíæ JSON: {json_file}")
        
        # Step 6: Show sample outputs
        print(f"\nüîç Step 6: Sample PDBML XML preview")
        print("=" * 40)
        
        # Show formatted sample (first 800 characters)
        sample_xml = xml_content[:800]
        if len(xml_content) > 800:
            sample_xml += "\n    ... (truncated)"
        print(sample_xml)
        
        if comprehensive and 'nested_json' in locals():
            print(f"\nüîç Sample nested JSON relationships:")
            sample_json = {}
            for cat_name, cat_data in list(nested_json.items())[:2]:
                if isinstance(cat_data, dict):
                    sample_json[cat_name] = {}
                    for key, item in list(cat_data.items())[:1]:
                        if isinstance(item, dict):
                            # Show simplified item with first few fields
                            simplified = {}
                            field_count = 0
                            for field, value in item.items():
                                if field_count < 3 and not isinstance(value, list):
                                    simplified[field] = value
                                    field_count += 1
                                elif isinstance(value, list):
                                    simplified[field] = f"[{len(value)} items]"
                            sample_json[cat_name][key] = simplified
                    if len(cat_data) > 1:
                        sample_json[cat_name]["..."] = f"and {len(cat_data) - 1} more items"
            
            print(json.dumps(sample_json, indent=2))
        
        # Step 7: Summary
        print(f"\nüìä Pipeline Summary")
        print("=" * 40)
        print(f"‚úÖ mmCIF parsing: SUCCESS")
        print(f"‚úÖ PDBML XML generation: SUCCESS")
        print(f"{'‚úÖ' if is_valid else '‚ö†Ô∏è'} Schema validation: {'SUCCESS' if is_valid else 'WITH WARNINGS'}")
        
        if comprehensive:
            print(f"‚úÖ Relationship resolution: SUCCESS")
            print(f"‚úÖ Nested JSON output: SUCCESS")
        
        print(f"\nüéØ Key achievements:")
        print(f"   ‚Ä¢ Parsed {len(container.data[0].categories)} mmCIF categories")
        print(f"   ‚Ä¢ Generated PDBML XML conforming to pdbx-v50.xsd")
        print(f"   ‚Ä¢ {'Perfect XML compliance!' if is_valid else 'Minor validation warnings only'}")
        
        if comprehensive:
            print(f"   ‚Ä¢ Correctly placed key fields as XML attributes")
            print(f"   ‚Ä¢ Resolved parent-child relationships")
            print(f"   ‚Ä¢ Created hierarchical JSON with nested structures")
        
        print(f"\nüìÅ All outputs saved to: {output_dir}/")
        
    except Exception as e:
        print(f"‚ùå Error in PDBML pipeline: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Cleanup demo files
        if os.path.exists(demo_file):
            os.remove(demo_file)


def demonstrate_complete_pdbml_pipeline():
    """Run the complete PDBML pipeline using the MMCIFToPDBMLPipeline class."""
    print("\nüöÄ Complete PDBML Pipeline Demo")
    print("=" * 40)
    print("üìä Using the integrated MMCIFToPDBMLPipeline class")
    
    # Create demo data
    demo_file = create_pdbml_demo_data()
    
    try:
        # Initialize pipeline
        schema_path = Path(__file__).parent / "sloth" / "schemas" / "pdbx-v50.xsd"
        dict_path = Path(__file__).parent / "sloth" / "schemas" / "mmcif_pdbx_v50.dic"
        
        if not schema_path.exists():
            print("‚ùå Schema file not found - pipeline cannot validate")
            return
        
        if not dict_path.exists():
            print("‚ùå Dictionary file not found - using basic conversion")
            pipeline = MMCIFToPDBMLPipeline(schema_path=schema_path)
        else:
            pipeline = MMCIFToPDBMLPipeline(schema_path=schema_path, dictionary_path=dict_path)
        
        print("‚úÖ Pipeline initialized")
        
        # Run complete pipeline
        print(f"\nüîÑ Running complete pipeline on {demo_file}")
        result = pipeline.process_mmcif_file(demo_file)
        
        # Display results
        print(f"\nüìä Pipeline Results:")
        print(f"   ‚úÖ mmCIF parsing: SUCCESS")
        print(f"   ‚úÖ XML generation: SUCCESS")
        print(f"   {'‚úÖ' if result['validation']['is_valid'] else '‚ö†Ô∏è'} Schema validation: {'PASSED' if result['validation']['is_valid'] else 'FAILED'}")
        print(f"   üìù Validation errors: {len(result['validation']['errors'])}")
        
        if result['validation']['errors']:
            print(f"   üîç First 3 validation errors:")
            for i, error in enumerate(result['validation']['errors'][:3], 1):
                print(f"      {i}. {error}")
        
        print(f"   ‚úÖ Relationship resolution: SUCCESS")
        print(f"   üìä Root categories in JSON: {len(result['nested_json']) if result['nested_json'] else 0}")
        
        # Save outputs
        output_dir = Path("exports") / "complete_pdbml_demo"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        file_paths = pipeline.save_outputs(result, output_dir, "complete_demo")
        
        # Save XML
        if result['pdbml_xml']:
            print(f"   üíæ XML saved: {file_paths['xml']}")
        
        # Save JSON
        if result['nested_json']:
            print(f"   üíæ JSON saved: {file_paths['json']}")
        
        # Save pipeline report
        print(f"   üìã Validation report: {file_paths['validation']}")
        
        print(f"\nüìÅ All outputs saved to: {output_dir}/")
        print(f"üéâ Complete pipeline demonstration finished successfully!")
        
    except Exception as e:
        print(f"‚ùå Error in complete pipeline: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Cleanup
        if os.path.exists(demo_file):
            os.remove(demo_file)


def demonstrate_nested_relationships():
    """Demonstrate the multi-level nested relationship resolution functionality."""
    print("\nüß¨ Nested Relationship Resolution Demo")
    print("=" * 50)
    print("üìä Testing 4-level hierarchical parent-child relationship resolution")
    
    # Create test data with complex nested relationships
    nested_content = """data_NESTED_DEMO
#
_entry.id        NESTED_DEMO
#
_entity.id       1
_entity.type     polymer
_entity.pdbx_description 'Test protein with complex relationships'
#
_entity_poly.entity_id 1
_entity_poly.type      'polypeptide(L)'
_entity_poly.nstd_chirality no
#
_entity_poly_seq.entity_id 1
_entity_poly_seq.num       1
_entity_poly_seq.mon_id    VAL
#
_struct_asym.id      A
_struct_asym.entity_id 1
#
loop_
_atom_type.symbol
_atom_type.number_in_cell
_atom_type.scat_dispersion_real
_atom_type.scat_dispersion_imag
C 1 0.0033 0.0016
#
_atom_site.group_PDB  ATOM
_atom_site.id         1
_atom_site.type_symbol C
_atom_site.label_atom_id CA
_atom_site.label_comp_id VAL
_atom_site.label_asym_id A
_atom_site.label_entity_id 1
_atom_site.label_seq_id 1
_atom_site.Cartn_x    12.345
_atom_site.Cartn_y    67.890
_atom_site.Cartn_z    42.000
_atom_site.occupancy  1.00
_atom_site.B_iso_or_equiv 35.0
_atom_site.pdbx_PDB_model_num 1
#"""
    
    test_file = 'nested_demo.cif'
    
    try:
        # Create test file
        with open(test_file, 'w') as f:
            f.write(nested_content)
        print(f"üìù Created test file: {test_file}")
        
        # Step 1: Parse mmCIF
        print(f"\n1Ô∏è‚É£ Parsing mmCIF with nested structures...")
        parser = MMCIFParser()
        container = parser.parse_file(test_file)
        print(f"   ‚úÖ Parsed successfully")
        print(f"   üìã Categories: {list(container.data[0].categories)}")
        
        # Step 2: Convert to PDBML XML
        print(f"\n2Ô∏è‚É£ Converting to PDBML XML...")
        dict_path = Path(__file__).parent / "sloth" / "schemas" / "mmcif_pdbx_v50.dic"
        converter = PDBMLConverter(dictionary_path=dict_path)
        xml_content = converter.convert_to_pdbml(container)
        print(f"   ‚úÖ XML generated - {len(xml_content)} characters")
        
        # Step 3: Resolve relationships using dictionary-driven approach
        print(f"\n3Ô∏è‚É£ Resolving parent-child relationships...")
        dictionary = DictionaryParser()
        dictionary.parse_dictionary(dict_path)
        resolver = RelationshipResolver(dictionary)
        nested_json = resolver.resolve_relationships(xml_content)
        print(f"   ‚úÖ Relationships resolved")
        print(f"   üìä Root categories: {list(nested_json)}")
        
        # Step 4: Validate 4-level hierarchy
        print(f"\n4Ô∏è‚É£ Validating 4-level nested hierarchy...")
        try:
            # Navigate the expected hierarchy
            entity_1 = nested_json['entity']['1']
            print(f"   üì¶ Level 1 - Entity: {entity_1['type']}")
            
            # Branch 1: entity -> entity_poly -> entity_poly_seq
            entity_poly = entity_1['entity_poly']
            print(f"   üß¨ Level 2 - Entity_poly: {entity_poly['type']}")
            
            entity_poly_seq = entity_poly['entity_poly_seq']
            print(f"   üîó Level 3 - Entity_poly_seq: {entity_poly_seq['mon_id']}")
            
            # Branch 2: entity -> struct_asym -> atom_site
            struct_asym = entity_1['struct_asym']
            print(f"   üèóÔ∏è Level 2 - Struct_asym: {struct_asym['id']}")
            
            atom_site = struct_asym['atom_site']
            print(f"   ‚öõÔ∏è Level 3 - Atom_site: {atom_site['label_atom_id']} at {atom_site['Cartn_x']}")
            
            print(f"   ‚úÖ 4-level hierarchy validated successfully!")
            
            # Step 5: Show relationship structure
            print(f"\n5Ô∏è‚É£ Relationship structure analysis:")
            print(f"   entity(1)")
            print(f"   ‚îú‚îÄ‚îÄ entity_poly")
            print(f"   ‚îÇ   ‚îî‚îÄ‚îÄ entity_poly_seq (VAL)")
            print(f"   ‚îî‚îÄ‚îÄ struct_asym(A)")
            print(f"       ‚îî‚îÄ‚îÄ atom_site (CA at 12.345, 67.890, 42.000)")
            
            # Step 6: Save outputs
            print(f"\n6Ô∏è‚É£ Saving demonstration outputs...")
            output_dir = Path("nested_demo_output")
            output_dir.mkdir(exist_ok=True)
            
            # Save XML
            xml_file = output_dir / "nested_demo.xml"
            with open(xml_file, 'w') as f:
                f.write(xml_content)
            print(f"   üíæ XML: {xml_file}")
            
            # Save nested JSON
            json_file = output_dir / "perfect_nested_structure.json"
            with open(json_file, 'w') as f:
                json.dump(nested_json, f, indent=2)
            print(f"   üíæ JSON: {json_file}")
            
            # Create ideal structure visualization
            ideal_structure = {
                "description": "4-level nested hierarchy demonstration",
                "hierarchy": {
                    "entity": {
                        "1": {
                            "type": "polymer",
                            "description": entity_1['pdbx_description'],
                            "entity_poly": {
                                "type": entity_poly['type'],
                                "entity_poly_seq": {
                                    "num": entity_poly_seq['num'],
                                    "mon_id": entity_poly_seq['mon_id']
                                }
                            },
                            "struct_asym": {
                                "id": struct_asym['id'],
                                "atom_site": {
                                    "atom": atom_site['label_atom_id'],
                                    "coordinates": [
                                        atom_site['Cartn_x'],
                                        atom_site['Cartn_y'],
                                        atom_site['Cartn_z']
                                    ]
                                }
                            }
                        }
                    }
                },
                "validation": {
                    "levels": 4,
                    "branches": 2,
                    "status": "SUCCESS"
                }
            }
            
            ideal_file = output_dir / "ideal_nested_structure.json"
            with open(ideal_file, 'w') as f:
                json.dump(ideal_structure, f, indent=2)
            print(f"   üíæ Ideal structure: {ideal_file}")
            
            print(f"\nüéâ Nested relationship demonstration completed successfully!")
            print(f"üí° Key achievements:")
            print(f"   ‚Ä¢ Correctly parsed complex mmCIF relationships")
            print(f"   ‚Ä¢ Generated valid PDBML XML with proper nesting")
            print(f"   ‚Ä¢ Resolved 4-level parent-child hierarchy")
            print(f"   ‚Ä¢ entity ‚Üí entity_poly ‚Üí entity_poly_seq")
            print(f"   ‚Ä¢ entity ‚Üí struct_asym ‚Üí atom_site")
            print(f"   ‚Ä¢ Preserved all data integrity and cross-references")
            
            return True
            
        except (KeyError, TypeError) as e:
            print(f"   ‚ùå Hierarchy validation failed: {e}")
            print(f"   üîç Available structure: {json.dumps(nested_json, indent=2)[:500]}...")
            return False
            
    except Exception as e:
        print(f"‚ùå Error in nested relationship demo: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # Cleanup
        if os.path.exists(test_file):
            os.remove(test_file)


def main():
    parser = argparse.ArgumentParser(
        description="SLOTH - Structural Loader with On-demand Traversal Handling | Lazy by design. Fast by default.",
        epilog="""
Examples:
  python demo.py input.cif output.cif
  python demo.py input.cif output.cif --categories _database_2 _atom_site
  python demo.py input.cif output.cif --validate
  python demo.py --demo  # Run comprehensive demo including PDBML pipeline
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument("input", nargs="?", help="Path to input mmCIF file")
    parser.add_argument("output", nargs="?", help="Path to write modified mmCIF file")
    parser.add_argument(
        "--categories", nargs="+", help="Specific categories to process", default=None
    )
    parser.add_argument(
        "--validate", action="store_true", help="Run validation on categories"
    )
    # Removed --schema-validate flag as it's always included in demo mode
    parser.add_argument("--demo", action="store_true", help="Run comprehensive demo with sample data (includes PDBML pipeline)")

    args = parser.parse_args()

    # Handle demo mode
    if args.demo:
        print("ü¶• SLOTH Demo")
        print("Lazy by design. Fast by default.")
        print("=" * 40)
        print("‚ö° Now using gemmi backend by default for high-performance parsing!")
        print("   Same elegant API, optimal performance")
        print("   Legacy implementations available in sloth.legacy")
        print()

        sample_file = demo_with_sample_file()
        args.input = sample_file
        args.output = "demo_modified.cif"
        args.validate = True
        args.schema_validate = True

    # Validate arguments
    if not args.input or not args.output:
        if not args.demo:
            parser.error("Both input and output files are required (or use --demo)")

    # Check input file exists
    if not os.path.exists(args.input):
        print(f"‚ùå Error: Input file '{args.input}' not found")
        return 1

    print(f"\nüîç Processing: {args.input}")
    if args.categories:
        print(f"üìÇ Categories: {', '.join(args.categories)}")

    # Setup handler
    validator_factory = ValidatorFactory() if args.validate else None
    handler = MMCIFHandler(validator_factory=validator_factory)
    
    print("‚ö° Using gemmi backend for high-performance parsing")

    try:
        # Parse the file
        print("‚ö° Parsing file...")
        mmcif = handler.parse(args.input, categories=args.categories)

        # Show file information
        show_file_info(mmcif)

        # Demonstrate sample data creation methods (in demo mode)
        if args.demo:
            demonstrate_sample_data_creation()

            # Demonstrate the auto-creation feature
            demonstrate_auto_creation()

        # Setup validation if requested
        if args.validate and mmcif.data:
            print(f"\nüõ°Ô∏è  Setting up validation...")
            block = mmcif.data[0]

            # Register validators for available categories
            available_categories = block.categories[:2]  # First 2 for demo
            for cat_name in available_categories:
                validator_factory.register_validator(cat_name, category_validator)

            # Register cross-checker if we have multiple categories
            if len(available_categories) >= 2:
                cat_pair = (available_categories[0], available_categories[1])
                validator_factory.register_cross_checker(cat_pair, cross_checker)

            # Run validation
            print(f"üîç Running validation...")
            for cat_name in available_categories:
                if cat_name in block.categories:
                    category = block.data[cat_name]
                    category.validate()

            # Run cross-validation if available
            if len(available_categories) >= 2:
                cat1_name, cat2_name = available_categories[0], available_categories[1]
                if cat1_name in block.categories and cat2_name in block.categories:
                    cat1 = block.data[cat1_name]
                    cat2 = block.data[cat2_name]
                    cat1.validate().against(cat2)

        # Demonstrate 2D slicing
        demonstrate_2d_slicing(mmcif)

        # Modify data
        print(f"\n‚úèÔ∏è  Modifying data...")
        modify_data(mmcif)

        # Write output
        print(f"\nüíæ Writing to: {args.output}")
        with open(args.output, "w") as f:
            handler.file_obj = f
            handler.write(mmcif)

        print(f"‚úÖ Successfully processed!")

        # Verify the output
        print(f"\nüîç Verifying output...")
        verify_data = handler.parse(args.output)
        print(f"‚úÖ Output file contains {len(verify_data.data)} data block(s)")

        # Demonstrate 2D slicing if available
        if hasattr(handler, "demonstrate_2d_slicing"):
            demonstrate_2d_slicing(mmcif)

        # Demonstrate export functionality
        output_dir = "exports"
        demonstrate_export_functionality(mmcif, output_dir)

        # Demonstrate import functionality
        imported_containers = demonstrate_import_functionality(output_dir)
        
        # Demonstrate gemmi comparison in demo mode
        if args.demo:
            demo_backend_comparison(args.input)

        # Demonstrate round-trip validation for each imported format
        for format_name, imported_container in imported_containers.items():
            demonstrate_round_trip(mmcif, imported_container, format_name)

        # Demonstrate schema validation
        # Note: This is always included in demo mode
        validation_dir = demonstrate_schema_validation(mmcif, output_dir)

        # Demonstrate PDBML pipeline (in demo mode)
        if args.demo:
            print("\n" + "=" * 60)
            print("üß¨ PDBML CONVERSION PIPELINE DEMONSTRATION")
            print("=" * 60)
            print("Now demonstrating the complete PDBML conversion pipeline!")
            print("This shows mmCIF ‚Üí PDBML XML ‚Üí Validation ‚Üí Relationship Resolution")
            
            # Run basic PDBML demo
            demonstrate_pdbml_pipeline(comprehensive=False)
            
            # Run nested relationship demo
            demonstrate_nested_relationships()
            
            # Run comprehensive PDBML demo  
            demonstrate_complete_pdbml_pipeline()

        # Clean up demo files if created
        if args.demo and os.path.exists("demo_structure.cif"):
            os.remove("demo_structure.cif")
            print("üßπ Cleaned up demo files")

        # Clean up validation examples
        if "validation_dir" in locals() and os.path.exists(validation_dir):
            try:
                shutil.rmtree(validation_dir)
                print("üßπ Cleaned up validation example files")
            except Exception:
                pass

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
